---
title: "Search"
description: "Search the web and get full content from results"
og:title: "Search | Firecrawl"
og:description: "Search the web and get full page content from results"
icon: "magnifying-glass"
---

import InstallationPython from "/snippets/v1/installation/python.mdx";
import InstallationNode from "/snippets/v1/installation/js.mdx";
import InstallationGo from "/snippets/v1/installation/go.mdx";
import InstallationRust from "/snippets/v1/installation/rust.mdx";
import SearchPython from "/snippets/v1/search/base/python.mdx";
import SearchNode from "/snippets/v1/search/base/js.mdx";
import SearchCURL from "/snippets/v1/search/base/curl.mdx";
import SearchContentPython from "/snippets/v1/search/content/python.mdx";
import SearchContentNode from "/snippets/v1/search/content/js.mdx";
import SearchContentCURL from "/snippets/v1/search/content/curl.mdx";
import SearchLocationPython from "/snippets/v1/search/location/python.mdx";
import SearchLocationNode from "/snippets/v1/search/location/js.mdx";
import SearchLocationCURL from "/snippets/v1/search/location/curl.mdx";
import SearchTimePython from "/snippets/v1/search/time/python.mdx";
import SearchTimeNode from "/snippets/v1/search/time/js.mdx";
import SearchTimeCURL from "/snippets/v1/search/time/curl.mdx";
import SearchOptimizationPython from "/snippets/v1/search/optimization/python.mdx";
import SearchOptimizationNode from "/snippets/v1/search/optimization/js.mdx";
import SearchOptimizationCURL from "/snippets/v1/search/optimization/curl.mdx";
import SearchPDFPython from "/snippets/v1/search/pdf/python.mdx";
import SearchPDFNode from "/snippets/v1/search/pdf/js.mdx";
import SearchPDFCURL from "/snippets/v1/search/pdf/curl.mdx";
import SearchBlockedSitesPython from "/snippets/v1/search/blocked-sites/python.mdx";
import SearchBlockedSitesNode from "/snippets/v1/search/blocked-sites/js.mdx";
import SearchBlockedSitesCURL from "/snippets/v1/search/blocked-sites/curl.mdx";

Firecrawl's search API allows you to perform web searches and optionally scrape the search results in one operation.

- Choose specific output formats (markdown, HTML, links, screenshots)
- Search the web with customizable parameters (location, etc.)
- Optionally retrieve content from search results in various formats
- Control the number of results and set timeouts

For details, see the [Search Endpoint API Reference](https://docs.firecrawl.dev/api-reference/endpoint/search).

## Performing a Search with Firecrawl

### /search endpoint

Used to perform web searches and optionally retrieve content from the results.

### Installation 

<CodeGroup>

  <InstallationPython />

  <InstallationNode />

  <InstallationGo />
  
  <InstallationRust />

</CodeGroup>

### Basic Usage

<CodeGroup>

  <SearchPython />

  <SearchNode />
  
  <SearchCURL />

</CodeGroup>

### Response

SDKs will return the data object directly. cURL will return the complete payload.

```json
{
  "success": true,
  "data": [
    {
      "title": "Firecrawl - The Ultimate Web Scraping API",
      "description": "Firecrawl is a powerful web scraping API that turns any website into clean, structured data for AI and analysis.",
      "url": "https://firecrawl.dev/"
    },
    {
      "title": "Web Scraping with Firecrawl - A Complete Guide",
      "description": "Learn how to use Firecrawl to extract data from websites effortlessly.",
      "url": "https://firecrawl.dev/guides/web-scraping/"
    },
    {
      "title": "Firecrawl Documentation - Getting Started",
      "description": "Official documentation for the Firecrawl web scraping API.",
      "url": "https://docs.firecrawl.dev/"
    }
    // ... more results
  ]
}
```

## Search with Content Scraping

Search and retrieve content from the search results in one operation.

<CodeGroup>

  <SearchContentPython />

  <SearchContentNode />
  
  <SearchContentCURL />

</CodeGroup>

### Response with Scraped Content

```json
{
  "success": true,
  "data": [
    {
      "title": "Firecrawl - The Ultimate Web Scraping API",
      "description": "Firecrawl is a powerful web scraping API that turns any website into clean, structured data for AI and analysis.",
      "url": "https://firecrawl.dev/",
      "markdown": "# Firecrawl\n\nThe Ultimate Web Scraping API\n\n## Turn any website into clean, structured data\n\nFirecrawl makes it easy to extract data from websites for AI applications, market research, content aggregation, and more...",
      "links": [
        "https://firecrawl.dev/pricing",
        "https://firecrawl.dev/docs",
        "https://firecrawl.dev/guides",
        // ... more links
      ],
      "metadata": {
        "title": "Firecrawl - The Ultimate Web Scraping API",
        "description": "Firecrawl is a powerful web scraping API that turns any website into clean, structured data for AI and analysis.",
        "sourceURL": "https://firecrawl.dev/",
        "statusCode": 200
      }
    },
    // ... more results
  ]
}
```

## Advanced Search Options

Firecrawl's search API supports various parameters to customize your search:

### Location Customization

<CodeGroup>

  <SearchLocationPython />

  <SearchLocationNode />
  
  <SearchLocationCURL />

</CodeGroup>

### Time-Based Search

Use the `tbs` parameter to filter results by time:

<CodeGroup>

  <SearchTimePython />

  <SearchTimeNode />
  
  <SearchTimeCURL />

</CodeGroup>

Common `tbs` values:
- `qdr:h` - Past hour
- `qdr:d` - Past 24 hours
- `qdr:w` - Past week
- `qdr:m` - Past month
- `qdr:y` - Past year

### Custom Timeout

Set a custom timeout for search operations:

<CodeGroup>

```python Python
from firecrawl import FirecrawlApp

# Initialize the client with your API key
app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Set a 30-second timeout
search_result = app.search(
    "complex search query",
    limit=10,
    timeout=30000  # 30 seconds in milliseconds
)
```

```js JavaScript
import FirecrawlApp from '@mendable/firecrawl-js';

// Initialize the client with your API key
const app = new FirecrawlApp({apiKey: "fc-YOUR_API_KEY"});

// Set a 30-second timeout
app.search("complex search query", {
  limit: 10,
  timeout: 30000  // 30 seconds in milliseconds
})
.then(searchResult => {
  // Process results
  console.log(searchResult.data);
});
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v1/search \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer fc-YOUR_API_KEY" \
  -d '{
    "query": "complex search query",
    "limit": 10,
    "timeout": 30000
  }'
```

</CodeGroup>

## Scraping Options

When scraping search results, you can specify multiple output formats:

<CodeGroup>

```python Python
from firecrawl import FirecrawlApp, ScrapeOptions

# Initialize the client with your API key
app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Get search results with multiple formats
search_result = app.search(
    "firecrawl features",
    limit=3,
    scrape_options=ScrapeOptions(formats=["markdown", "html", "links", "screenshot"])
)
```

```js JavaScript
import FirecrawlApp from '@mendable/firecrawl-js';

// Initialize the client with your API key
const app = new FirecrawlApp({apiKey: "fc-YOUR_API_KEY"});

// Get search results with multiple formats
app.search("firecrawl features", {
  limit: 3,
  scrapeOptions: {
    formats: ["markdown", "html", "links", "screenshot"]
  }
})
.then(searchResult => {
  // Process results with various formats
  console.log(searchResult.data);
});
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v1/search \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer fc-YOUR_API_KEY" \
  -d '{
    "query": "firecrawl features",
    "limit": 3,
    "scrapeOptions": {
      "formats": ["markdown", "html", "links", "screenshot"]
    }
  }'
```

</CodeGroup>

Available formats:
- `markdown`: Clean, formatted markdown content
- `html`: Processed HTML content
- `rawHtml`: Unmodified HTML content
- `links`: List of links found on the page
- `screenshot`: Screenshot of the page
- `screenshot@fullPage`: Full-page screenshot
- `extract`: Structured data extraction

For more details about format options, refer to the [Scrape Feature documentation](https://docs.firecrawl.dev/features/scrape).

## Cost Optimization

### Base Search Costs

The search endpoint charges **1 credit per search result**, controlled by the `limit` parameter. For example:
- `limit: 5` = 5 credits for the search operation
- `limit: 10` = 10 credits for the search operation

### Avoiding Content Scraping

To perform searches without scraping content (which consumes additional credits), you have two options:

1. **Empty formats array**: Provide `scrapeOptions: {formats: []}` to get only basic search results without scraping
2. **Omit scrapeOptions entirely**: This returns basic search results AND includes sites that are normally blocked from scraping

<CodeGroup>

  <SearchOptimizationPython />

  <SearchOptimizationNode />
  
  <SearchOptimizationCURL />

</CodeGroup>

**Cost Impact**: When avoiding content scraping, you only pay for the base search operation (1 credit per result). No additional credits are consumed for content scraping.

### Including Blocked Sites

To include sites that are normally blocked from scraping in your search results, omit the `scrapeOptions` parameter entirely:

<CodeGroup>

  <SearchBlockedSitesPython />

  <SearchBlockedSitesNode />
  
  <SearchBlockedSitesCURL />

</CodeGroup>

**Note**: This approach returns only basic search results (title, URL, description) but includes sites that would normally be filtered out when `scrapeOptions` is present.

### PDF Pricing Optimization

When search results include PDF files AND you're scraping content (not search-only), you can control PDF billing costs using the `parsePDF` parameter:

- **Default (`parsePDF: true`)**: 1 credit per PDF page (in addition to base search cost and scraping cost)
- **Optimized (`parsePDF: false`)**: 1 credit total per PDF document (in addition to base search cost and scraping cost)

<CodeGroup>

  <SearchPDFPython />

  <SearchPDFNode />
  
  <SearchPDFCURL />

</CodeGroup>

**Cost Comparison**: A 50-page PDF would cost 50 additional credits with `parsePDF: true` (default) but only 1 additional credit with `parsePDF: false`. The trade-off is that with `parsePDF: false`, you get the PDF in base64 encoding instead of extracted text content.
