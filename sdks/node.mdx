---
title: "Node"
description: "Firecrawl Node SDK is a wrapper around the Firecrawl API to help you easily turn websites into markdown."
icon: "node"
og:title: "Node SDK | Firecrawl"
og:description: "Firecrawl Node SDK is a wrapper around the Firecrawl API to help you easily turn websites into markdown."
---

import InstallationNode from "/snippets/v2/installation/js.mdx";
import ScrapeAndCrawlExampleNode from "/snippets/v2/scrape-and-crawl/js.mdx";
import ScrapeNodeShort from "/snippets/v2/scrape/short/js.mdx";
import CrawlNodeShort from "/snippets/v2/crawl/short/js.mdx";
import StartCrawlNodeShort from "/snippets/v2/start-crawl/short/js.mdx";
import CheckCrawlStatusNodeShort from "/snippets/v2/crawl-status/short/js.mdx";
import CancelCrawlNodeShort from "/snippets/v2/crawl-delete/short/js.mdx";
import MapNodeShort from "/snippets/v2/map/short/js.mdx";
import ExtractNodeShort from "/snippets/v2/extract/short/js.mdx";
import CrawlWebSocketNodeBase from "/snippets/v2/crawl-websocket/base/js.mdx";

## Installation

To install the Firecrawl Node SDK, you can use npm:

<InstallationNode />

## Usage

1. Get an API key from [firecrawl.dev](https://firecrawl.dev)
2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` class.

Here's an example of how to use the SDK with error handling:

<ScrapeAndCrawlExampleNode />

### Scraping a URL

To scrape a single URL with error handling, use the `scrapeUrl` method. It takes the URL as a parameter and returns the scraped data as a dictionary.

<ScrapeNodeShort />

### Crawling a Website

To crawl a website with error handling, use the `crawlUrl` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format. See [Pagination](#pagination) for auto/ manual pagination and limiting.

<CrawlNodeShort />

### Start a Crawl

Start a job without waiting using `startCrawl`. It returns a job `ID` you can use to check status. Use `crawl` when you want a waiter that blocks until completion. See [Pagination](#pagination) for paging behavior and limits.

<StartCrawlNodeShort />

### Checking Crawl Status

To check the status of a crawl job with error handling, use the `checkCrawlStatus` method. It takes the `ID` as a parameter and returns the current status of the crawl job.

<CheckCrawlStatusNodeShort />

### Cancelling a Crawl

To cancel an crawl job, use the `cancelCrawl` method. It takes the job ID of the `startCrawl` as a parameter and returns the cancellation status.

<CancelCrawlNodeShort />

### Mapping a Website

To map a website with error handling, use the `mapUrl` method. It takes the starting URL as a parameter and returns the mapped data as a dictionary.

<MapNodeShort />

{/\* ### Extracting Structured Data from Websites

To extract structured data from websites with error handling, use the `extractUrl` method. It takes the starting URL as a parameter and returns the extracted data as a dictionary.

<ExtractNodeShort /> */}

### Crawling a Website with WebSockets

To crawl a website with WebSockets, use the `crawlUrlAndWatch` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.

<CrawlWebSocketNodeBase />

### Pagination

Firecrawl endpoints for crawl and batch return a `next` URL when more data is available. The Node SDK auto-paginates by default and aggregates all documents; in that case `next` will be `null`. You can disable auto-pagination or set limits.

#### Crawl

Use the waiter method `crawl` for the simplest experience, or start a job and page manually.

##### Simple crawl (auto-pagination, default)

- See the default flow in [Crawling a Website](#crawling-a-website).

##### Manual crawl with pagination control (single page)

- Start a job, then fetch one page at a time with `autoPaginate: false`.

```js Node
const crawlStart = await firecrawl.startCrawl("https://docs.firecrawl.dev", {
  limit: 5,
});
const crawlJobId = crawlStart.id;

const crawlSingle = await firecrawl.getCrawlStatus(crawlJobId, {
  autoPaginate: false,
});
console.log(
  "crawl single page:",
  crawlSingle.status,
  "docs:",
  crawlSingle.data.length,
  "next:",
  crawlSingle.next,
);
```

##### Manual crawl with limits (auto-pagination + early stop)

- Keep auto-pagination on but stop early with `maxPages`, `maxResults`, or `maxWaitTime`.

```js Node
const crawlLimited = await firecrawl.getCrawlStatus(crawlJobId, {
  autoPaginate: true,
  maxPages: 2,
  maxResults: 50,
  maxWaitTime: 15,
});
console.log(
  "crawl limited:",
  crawlLimited.status,
  "docs:",
  crawlLimited.data.length,
  "next:",
  crawlLimited.next,
);
```

#### Batch Scrape

Use the waiter method `batchScrape`, or start a job and page manually.

##### Simple batch scrape (auto-pagination, default)

- See the default flow in [Batch Scrape](/features/batch-scrape).

##### Manual batch scrape with pagination control (single page)

- Start a job, then fetch one page at a time with `autoPaginate: false`.

```js Node
const batchStart = await firecrawl.startBatchScrape(
  ["https://docs.firecrawl.dev", "https://firecrawl.dev"],
  { options: { formats: ["markdown"] } },
);
const batchJobId = batchStart.id;

const batchSingle = await firecrawl.getBatchScrapeStatus(batchJobId, {
  autoPaginate: false,
});
console.log(
  "batch single page:",
  batchSingle.status,
  "docs:",
  batchSingle.data.length,
  "next:",
  batchSingle.next,
);
```

##### Manual batch scrape with limits (auto-pagination + early stop)

- Keep auto-pagination on but stop early with `maxPages`, `maxResults`, or `maxWaitTime`.

```js Node
const batchLimited = await firecrawl.getBatchScrapeStatus(batchJobId, {
  autoPaginate: true,
  maxPages: 2,
  maxResults: 100,
  maxWaitTime: 20,
});
console.log(
  "batch limited:",
  batchLimited.status,
  "docs:",
  batchLimited.data.length,
  "next:",
  batchLimited.next,
);
```

## Error Handling

The SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message. The examples above demonstrate how to handle these errors using `try/catch` blocks.
