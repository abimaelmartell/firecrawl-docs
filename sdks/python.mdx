---
title: 'Python'
description: 'Firecrawl Python SDK is a wrapper around the Firecrawl API to help you easily turn websites into markdown.'
icon: 'python'
og:title: "Python SDK | Firecrawl"
og:description: "Firecrawl Python SDK is a wrapper around the Firecrawl API to help you easily turn websites into markdown."
---

import InstallationPython from '/snippets/v2/installation/python.mdx'
import ScrapePythonShort from '/snippets/v2/scrape/short/python.mdx'
import CrawlPythonShort from '/snippets/v2/crawl/short/python.mdx'
import CheckCrawlStatusPythonShort from '/snippets/v2/crawl-status/short/python.mdx'
import StartCrawlPythonShort from '/snippets/v2/start-crawl/short/python.mdx'
import CancelCrawlPythonShort from '/snippets/v2/crawl-delete/short/python.mdx'
import MapPythonShort from '/snippets/v2/map/short/python.mdx'
import ExtractPythonShort from '/snippets/v2/extract/short/python.mdx'
import ScrapeAndCrawlExamplePython from '/snippets/v2/scrape-and-crawl/python.mdx'
import CrawlWebSocketPythonBase from '/snippets/v2/crawl-websocket/base/python.mdx'
import AIOPython from '/snippets/v2/async/base/python.mdx'

## Installation

To install the Firecrawl Python SDK, you can use pip:

<InstallationPython />

## Usage

1. Get an API key from [firecrawl.dev](https://firecrawl.dev)
2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `Firecrawl` class.


Here's an example of how to use the SDK:

<ScrapeAndCrawlExamplePython />

### Scraping a URL

To scrape a single URL, use the `scrape` method. It takes the URL as a parameter and returns the scraped document.

<ScrapePythonShort />

### Crawl a Website

To crawl a website, use the `crawl` method. It takes the starting URL and optional options as arguments. The options allow you to specify additional settings for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.

<CrawlPythonShort />

### Start a Crawl

<Tip>Prefer non-blocking? Check out the [Async Class](#async-class) section below.</Tip>

Start a job without waiting using `start_crawl`. It returns a job `ID` you can use to check status. Use `crawl` when you want a waiter that blocks until completion.

<StartCrawlPythonShort />


### Checking Crawl Status

To check the status of a crawl job, use the `get_crawl_status` method. It takes the job ID as a parameter and returns the current status of the crawl job.

<CheckCrawlStatusPythonShort />

### Cancelling a Crawl

To cancel an crawl job, use the `cancel_crawl` method. It takes the job ID of the `start_crawl` as a parameter and returns the cancellation status.

<CancelCrawlPythonShort />

### Map a Website

Use `map` to generate a list of URLs from a website. The options let you customize the mapping process, including excluding subdomains or utilizing the sitemap.

<MapPythonShort />

{/* ### Extracting Structured Data from Websites

To extract structured data from websites, use the `extract` method. It takes the URLs to extract data from, a prompt, and a schema as arguments. The schema is a Pydantic model that defines the structure of the extracted data.

<ExtractPythonShort /> */}

### Crawling a Website with WebSockets

To crawl a website with WebSockets, start the job with `start_crawl` and subscribe using the `watcher` helper. Create a watcher with the job ID and attach handlers (e.g., for page, completed, failed) before calling `start()`.

<CrawlWebSocketPythonBase />

## Error Handling

The SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message.

## Async Class

For async operations, use the `AsyncFirecrawl` class. Its methods mirror `Firecrawl`, but they don't block the main thread.

<AIOPython />
